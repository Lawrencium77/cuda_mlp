# Cuda MLP
Training an MLP on MNIST in raw CUDA.

## TODO

* Support non-square matmuls without getting a segfault.
* Cross-entropy loss.
* Backprop.

### Later On

* Faster operations, e.g. tiled matmuls.
